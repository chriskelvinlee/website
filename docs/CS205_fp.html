<html><head><title>CS205 FP Writeup Gibbs-Lee</title><style type="text/css">ol{margin:0;padding:0}p{margin:0}.c28{vertical-align:top;width:175.5pt;border-style:solid;border-color:#999999;border-width:1pt;padding:5pt 5pt 5pt 5pt}.c27{vertical-align:top;width:172.5pt;border-style:solid;border-color:#999999;border-width:1pt;padding:5pt 5pt 5pt 5pt}.c7{vertical-align:top;width:468pt;border-style:solid;border-color:#d9d9d9;border-width:1pt;padding:5pt 5pt 5pt 5pt}.c38{vertical-align:middle;width:172.5pt;border-style:solid;border-color:#999999;border-width:1pt;padding:5pt 5pt 5pt 5pt}.c21{vertical-align:top;width:234pt;border-style:solid;border-color:#d9d9d9;border-width:1pt;padding:5pt 5pt 5pt 5pt}.c31{list-style-type:lower-latin;margin:0;padding:0}.c30{list-style-type:disc;margin:0;padding:0}.c22{list-style-type:decimal;margin:0;padding:0}.c0{color:#cc4125;font-size:9pt;text-decoration:underline}.c14{line-height:1.0;text-indent:9pt;text-align:center}.c20{color:#b9400b;text-decoration:underline}.c34{text-align:right;padding-bottom:10pt}.c32{color:#000099;text-decoration:underline}.c44{width:468pt;padding:72pt 72pt 72pt 72pt}.c5{height:11pt;direction:ltr}.c13{color:#cc4125;text-decoration:underline}.c4{color:inherit;text-decoration:inherit}.c1{font-size:8pt;background-color:#ffffff}.c16{margin-left:72pt}.c3{font-size:8pt}.c6{font-size:10pt}.c11{margin-left:36pt}.c42{color:#666666}.c35{color:#cc4125}.c36{color:#bfbfbf}.c26{height:12pt}.c17{text-align:center}.c8{font-weight:bold}.c19{font-family:Verdana}.c43{font-size:18pt}.c29{line-height:1.0}.c40{border-collapse:collapse}.c41{margin-left:18pt}.c33{font-family:Courier New}.c10{direction:ltr}.c9{font-size:12pt}.c12{padding-left:0pt}.c23{font-style:italic}.c25{height:0pt}.c15{font-size:9pt}.c39{height:14pt}.c18{margin-left:54pt}.c24{font-size:14pt}.c37{color:#000099}.c2{background-color:#ffffff}body{color:#000000;font-size:11pt;font-family:Arial}h1{padding-top:24pt;color:#000000;font-size:24pt;font-family:Arial;font-weight:bold;padding-bottom:6pt}h2{padding-top:18pt;color:#000000;font-size:18pt;font-family:Arial;font-weight:bold;padding-bottom:4pt}h3{padding-top:14pt;color:#000000;font-size:14pt;font-family:Arial;font-weight:bold;padding-bottom:4pt}h4{padding-top:12pt;color:#000000;font-size:12pt;font-family:Arial;font-weight:bold;padding-bottom:2pt}h5{padding-top:11pt;color:#000000;font-size:11pt;font-family:Arial;font-weight:bold;padding-bottom:2pt}h6{padding-top:10pt;color:#000000;font-size:10pt;font-family:Arial;font-weight:bold;padding-bottom:2pt}</style></head><body class="c2 c44"><h1 class="c17 c10"><a name="h.3thqcs7ujaiw"></a><span>Robust Parallel Adaptive Smoothing</span></h1><p class="c5 c17"><span class="c6 c23"></span></p><p class="c17 c10"><span class="c6 c23">CS205 - Final Project</span></p><p class="c17 c10"><span class="c6 c23">NASA Chandra X-Ray Observatory</span></p><p class="c17 c10"><span class="c6 c23">December 4, 2011</span></p><p class="c5 c17"><span class="c6"></span></p><p class="c17 c10"><span class="c6 c8">Danny Gibbs</span></p><p class="c17 c10"><span class="c6">Harvard-Smithsonian Center for Astrophysics</span></p><p class="c17 c10"><span class="c6">dgibbs [at] head.cfa.harvard.edu </span></p><p class="c5 c17"><span class="c6"></span></p><p class="c17 c10"><span class="c6 c8">Christopher K. Lee</span></p><p class="c17 c10"><span class="c6">Harvard College</span></p><p class="c17 c10"><span class="c6">cklee [at] college.harvard.edu </span></p><p class="c5 c17"><span></span></p><p class="c5 c17"><span class="c35"></span></p><p class="c10"><span class="c32 c8"><a class="c4" href="https://github.com/chriskelvinlee/nasa_chandra">Download the Gibbs-Lee FP here</a></span><span class="c8 c37">&nbsp;</span><span>or </span><span>on the command line:</span></p><p class="c10"><span class="c33">$ git clone git@github.com:chriskelvinlee/nasa_chandra.git</span></p><p class="c5"><span class="c15 c2"></span></p><p class="c10 c41"><span class="c0"><a class="c4" href="#h.3thqcs7ujaiw">Robust Parallel Adaptive Smoothing</a></span></p><p class="c11 c10"><span class="c0"><a class="c4" href="#h.rsqksf9305hf">I. Purpose</a></span></p><p class="c10 c18"><span class="c0"><a class="c4" href="#h.gbt42bpw56qw">Abstract</a></span></p><p class="c16 c10"><span class="c0"><a class="c4" href="#h.ujcu5vr784dw">Background - &nbsp;Chandra X-ray Observatory</a></span></p><p class="c16 c10"><span class="c0"><a class="c4" href="#h.nkwt3dp1pa1t">Background - &nbsp;CIAO &amp; CALDB</a></span></p><p class="c16 c10"><span class="c0"><a class="c4" href="#h.3a64kwva2brg">Background - &nbsp;Adaptive Smoothing</a></span></p><p class="c10 c18"><span class="c0"><a class="c4" href="#h.xey2dgmb8993">Problem - Serial Version</a></span></p><p class="c10 c18"><span class="c0"><a class="c4" href="#h.gmmamn1zspfs">Solution - MPI-CUDA</a></span></p><p class="c11 c10"><span class="c0"><a class="c4" href="#h.7y9adig5v2wz">II. Data</a></span></p><p class="c10 c18"><span class="c0"><a class="c4" href="#h.a38yx9ycgqmx">Instrumentation</a></span></p><p class="c10 c18"><span class="c0"><a class="c4" href="#h.vhb0wb22jfqk">Our Datasets</a></span></p><p class="c16 c10"><span class="c0"><a class="c4" href="#h.yabxqp60psjx">Obtaining the Data</a></span></p><p class="c16 c10"><span class="c0"><a class="c4" href="#h.fbjskbfks9ov">Preparing the Data</a></span></p><p class="c10 c11"><span class="c0"><a class="c4" href="#h.z8a6np292i4p">III. Design</a></span></p><p class="c10 c18"><span class="c0"><a class="c4" href="#h.xrirg4e7pp4h">Preprocessing</a></span></p><p class="c10 c18"><span class="c0"><a class="c4" href="#h.573tyb65ad2a">Serial Impementation</a></span></p><p class="c10 c18"><span class="c0"><a class="c4" href="#h.lp9318555l">GPU Implementation</a></span></p><p class="c11 c10"><span class="c0"><a class="c4" href="#h.vixq8tcovk5">IV. Results</a></span></p><p class="c10 c18"><span class="c0"><a class="c4" href="#h.abknyabiouc1">Instructions</a></span></p><p class="c16 c10"><span class="c0"><a class="c4" href="#h.nfmjhw9rc8kx">Serial Instructions</a></span></p><p class="c16 c10"><span class="c0"><a class="c4" href="#h.jjsrbbvdvg4b">GPU Instructions</a></span></p><p class="c10 c18"><span class="c0"><a class="c4" href="#h.3jl7us3v8pcw">Results</a></span></p><p class="c16 c10"><span class="c0"><a class="c4" href="#h.dhjvdw535yjt">Sample Output</a></span></p><p class="c16 c10"><span class="c0"><a class="c4" href="#h.blvldm2lahe0">Speedup</a></span></p><p class="c10 c18"><span class="c0"><a class="c4" href="#h.c65oqio4bjn">Discussion</a></span></p><p class="c16 c10"><span class="c0"><a class="c4" href="#h.be8ny55ll768">Insights</a></span></p><p class="c11 c10"><span class="c0"><a class="c4" href="#h.59v0s8xxwkqs">V. Conclusion</a></span></p><p><span></span></p><p class="c5"><span></span></p><p class="c10"><span class="c8 c43">Adaptive Smoothing in Layman&rsquo;s Terms</span></p><p class="c10"><span class="c6 c2">The adaptive smoothing algorithm (ASMOOTH) is used widely in astronomy, medical imagery, animation rendering, and military applications to process data from sensor and adaptively filter noise by smoothing. The ASMOOTH is embarrassingly parallel and allows for refinement of micro-scale pixel features while maintaining macro-scale features over the image area.</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">Special thanks to Kenny Glotfelty of the Harvard-Smithsonian Center for Astrophysics (kjg [at] head.cfa.harvard.edu) for project idea and initial serial algorithm.</span></p><p class="c5"><span class="c15 c2"></span></p><hr><p class="c5"><span class="c15 c2"></span></p><p class="c5"><span class="c15 c2"></span></p><hr><p class="c5"><span class="c2"></span></p><h2 class="c10"><a name="h.rsqksf9305hf"></a><span>I. Purpose</span></h2><h3 class="c10"><a name="h.gbt42bpw56qw"></a><span>Abstract</span></h3><p class="c10"><span class="c6 c2">The purpose of this application is to parallelize the adaptive smoothing algorithm (ASMOOTH) in the NASA open-source CIAO toolkit and over the Chandra X-Ray Observatory data-set. A full resolution image file of a typical Chandra observation using the ACIS instrument is 8192x8192 pixels.</span><span class="c6 c2">&nbsp;The data value of a pixel correspond to the photon hits (X-ray source counts) and was held constant at </span><span class="c6 c8 c2">15 (Threshold)</span><span class="c6 c2">. The smoothing stencil size over pixel ranged from </span><span class="c6 c8 c2">n=1-4 (MaxRad)</span><span class="c6 c2">. The serial ASMOOTH algorithm completes within 30min. - 4.5 days, its complexity in worst case scenario is O(n^4), and requires 3 sets of for-loops that were embarrassingly parallellized into a 3 kernel MPI/CUDA implementation. We successfully managed shared/global memory bounds and observed logarithmic </span><span class="c6 c8 c2">speedup of 2.01-5.38x</span><span class="c6 c2">&nbsp;with </span><span class="c6 c8 c2">errors from 8.05e-06 to 1.30e-05</span><span class="c6 c2">. In other parameter regimes where Threshold = 2-10 and RadMax = 4-9, we observed that the CUDA implementation became unstable due to rounding errors producing errors from 7.21e-02 to 1.66e-03. We hope to submit this application for consideration for the CIAO Contributed Scripts and Modules that is </span><span class="c13 c6 c2"><a class="c4" href="http://cxc.harvard.edu/ciao/download/scripts/index.html">released</a></span><span class="c6 c2">&nbsp;with the CIAO 4.3.x software suite.</span></p><p class="c5"><span class="c15 c2"></span></p><h4 class="c10"><a name="h.ujcu5vr784dw"></a><span>Background - &nbsp;Chandra X-ray Observatory</span></h4><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">The Chandra X-ray Obervatory was launched by the NASA Columbia shuttle in 1999. Named after Indian-American physicists Subrahmanyan Cahndrasekhar, &ldquo;Chandra&rdquo; also means &ldquo;luminous&rdquo; in Sanskrit.</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">X-ray optic measurements are usually taken from space with satellite instrumentation to produce the best 2-dimensional angular resolution. This is because x-rays are absorbed in the Earth&rsquo;s atmosphere. &nbsp;So in order to collect x-ray data, the instrumentation needs to be above the atmosphere. &nbsp;As a result, the Chandra X-ray Observatory is located in orbit around the Earth above the interfering atmosphere. The Chandra X-ray Observatory has three major parts: (1) the X-ray telescope, whose mirrors focus X-rays from celestial objects; (2) the science instruments which record the X-rays so that X-ray images can be produced and analyzed; and (3) the spacecraft, which provides the environment necessary for the telescope and the instruments to work.</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">The instrument we will be focusing on in the is project is Advanced CCD Imaging Spectrometer (ACIS). This is one of two focal plane instruments. This instrument is an array of charged coupled devices (CCD&#39;s), which are sophisticated versions of the crude CCD&#39;s used in digital cameras. This instrument is especially useful because it can make X-ray images, and at the same time, measure the energy of each incoming X-ray. Thus scientists can make pictures of objects using only X-rays produced by a single chemical element, and so compare (for example) the appearance of a supernova remnant in light produced by oxygen ions to that of neon or iron ions. It is the instrument of choice for studying temperature variations across X-ray sources such as vast clouds of hot gas in intergalactic space, or chemical variations across clouds left by supernova explosions. &nbsp;</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6">ACIS Source: </span><span class="c20 c6"><a class="c4" href="http://chandra.si.edu/about/science_instruments.html">Harvard-Smithsonian Center for Astrophysics</a></span></p><h4 class="c10 c26"><a name="h.3w894o49xd27"></a></h4><h4 class="c10"><a name="h.nkwt3dp1pa1t"></a><span>Background - &nbsp;CIAO &amp; CALDB</span></h4><p class="c5"><span></span></p><p class="c10"><span class="c6 c2">Analytical software and algorithms are thus needed to produce accurate observations of Chandra observations. The Chandra X-Ray Observatory provides such software in the form of the &ldquo;Chandra Interactive Analysis of Observations&rdquo; or CIAO. &nbsp;CIAO is a flexible software suite that allows analysis of X-ray data collected from the Chandra Observatory as well as non-X-ray data from other missions. The CSC (Chandra Source Catalog) is a publicly accessible database that has combined data for most of the Chandra mission. One of the tools provided in the suite is called </span><span class="c6 c2 c13"><a class="c4" href="http://cxc.harvard.edu/ciao/ahelp/dmimgadapt.html">dmimgadapt()</a></span><span class="c6 c35 c2">&nbsp;</span><span class="c6 c2">which we will be used as a basis for the project.</span></p><p class="c10"><span class="c6 c2">&nbsp;</span></p><p class="c10"><span class="c6 c2">Source: </span><span class="c20 c6 c2"><a class="c4" href="http://cxc.harvard.edu/ciao/index.html">Harvard CXC</a></span></p><p class="c5"><span class="c6 c2"></span></p><h4 class="c10"><a name="h.3a64kwva2brg"></a><span>Background - &nbsp;Adaptive Smoothing</span></h4><p class="c5"><span class="c8 c9 c2"></span></p><table cellpadding="0" cellspacing="0" class="c40"><tbody><tr class="c25"><td class="c21"><p class="c17 c29 c10"><img height="243" src="images/image14.jpg" width="203"><span class="c8 c9 c2"><br></span><span class="c1 c8">Fig 1a</span></p></td><td class="c21"><p class="c17 c29 c10"><img height="243" src="images/image08.jpg" width="203"><span class="c8 c9 c2"><br></span><span class="c1 c8">Fig 1b</span></p></td></tr><tr class="c25"><td class="c21"><p class="c17 c29 c10"><img height="250" src="images/image02.jpg" width="209"><span class="c8 c9 c2"><br></span><span class="c1 c8">Fig 1c</span></p></td><td class="c21"><p class="c17 c29 c10"><img height="252" src="images/image10.jpg" width="211"><span class="c8 c9 c2"><br></span><span class="c1 c8">Fig 1d</span></p></td></tr></tbody></table><p class="c10"><span class="c1 c8">Figures 1a-d </span><span class="c1">Diagram of smoothing stencil size over one pixel for the &nbsp;Adaptive Smoothing Algorithm where n=1, 2, 3, and 9. Each pixel will has a unique stencil size n corresponding to the data value represented in the pixel within Threshold and MaxRad parameter bounds.</span></p><p class="c5"><span class="c8 c9 c2"></span></p><p class="c10"><span class="c6 c2">Adaptive smoothing of astronomical images enhances signal to noise ratio with minimum loss of effective resolution and keeping photometric fluxes unaltered.</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">While traditional smoothing methods use the same window shape and size to smooth an image independent of its content, in adaptive smoothing, the window shape and size are varied across the image domain depending on local image content. In adaptive smoothing a window is sized according to the local gradient magnitude and shaped in such a way that it has a shorter side across an edge compared to along the edge. This mechanism maintains edge details while smoothing random noise.</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">Source (Verbatim): </span><span class="c6 c2 c42">&nbsp;</span><span class="c13 c6 c2"><a class="c4" href="http://www.mpia.de/~zibetti/software/adaptsmooth.html">Max-Planck-Institut f&uuml;r Astronomie</a></span><span class="c6 c35 c2">, </span><span class="c13 c6 c2"><a class="c4" href="http://adsabs.harvard.edu/abs/2006MNRAS.368...65E">NASA</a></span><span class="c6 c2 c35">, </span><span class="c6 c2">and</span><span class="c6 c35 c2">&nbsp;</span><span class="c13 c6 c2"><a class="c4" href="http://www.imgfsr.com/ifsr_aism.html">Fusion Systems</a></span></p><h3 class="c10 c39"><a name="h.32af76mw25ic"></a></h3><h3 class="c10"><a name="h.xey2dgmb8993"></a><span>Problem - Serial Version</span></h3><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c8 c2">1). What is the problem you are trying to solve with this application?</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">The ASMOOTH algorithm is embarrassingly parallel. The problem is that the O(n^4) inefficiency and required three nested for-loops results in slow run-times. A full resolution image file of a typical Chandra observation using the ACIS instrument is 8192x8192 pixels. Due to time constraints and calculation complexities of algorithms that analyze the data, sometimes concessions need to be made. For example, CIAO provides a routine, </span><span class="c20 c6 c2"><a class="c4" href="http://goo.gl/qWBqe">dmimgadapt()</a></span><span class="c6 c2">, that takes an image as input then adaptively smooths the image. It does this by building a 2D box around each pixel until a user defined threshold is met, or a user defined max box size is encountered. The complexity of the algorithm in worst case scenario is O(n^4). Thus, with a large image, it can take days to achieve the output image. The data is usually scaled, or binned, by a factor of 8 in order to obtain a more manageable image size of 1024x1024. However, by doing so some of the finer detail of the image is lost. With this configuration, a typical file may take 30 minutes to produce an output file. The goal of the project is to modify the dmimgadapt() routine from C-code to a more general python implementation. Then we will implement a GPU version of the algorithm. When finished, we want to drastically improve the runtime of the algorithm, via the GPU, and test the ability to run the algorithm on a full resolution 8192x8192 image. Below is the psuedo code for the serial method:</span></p><p class="c5"><span></span></p><p class="c10"><span class="c3"># ----------------------</span></p><p class="c10"><span class="c3"># Psuedo Code for serial method</span></p><p class="c10"><span class="c3">#-----------------------</span></p><p class="c5"><span class="c3"></span></p><p class="c10"><span class="c3">LOAD image, IMG</span></p><p class="c5"><span class="c3"></span></p><p class="c10"><span class="c3">Let Lx = length of x-axis of IMG</span></p><p class="c10"><span class="c3">Let Ly = length of y-axis of IMG</span></p><p class="c5"><span class="c3"></span></p><p class="c10"><span class="c3">Let RAD = Lx, Ly float array init to 0</span></p><p class="c10"><span class="c3">Let TOTAL = Lx, Ly float array init to 0</span></p><p class="c10"><span class="c3">Let NORM = Lx, Ly float array init to 0</span></p><p class="c10"><span class="c3">Let OUT = Lx, Ly float array init to 0</span></p><p class="c5"><span class="c3"></span></p><p class="c10"><span class="c3">Let Threshold = min sum value</span></p><p class="c10"><span class="c3">Let MaxRad = maximun allowable box size</span></p><p class="c5"><span class="c3"></span></p><p class="c10"><span class="c3">#----------------------</span></p><p class="c10"><span class="c3">Let w[*][*] = 1.0</span></p><p class="c5"><span class="c3"></span></p><p class="c10"><span class="c3">for ( x = 0; x &lt; Lx; x++)</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; for(y=0; y&lt;Ly; y++)</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; q = 0</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; sum = 0</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; ksum = 0</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; s = q</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; While(sum &lt; Threshold) &amp;&amp; (q &lt; MaxRad)</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ss = q</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sum = 0</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ksum = 0</span></p><p class="c5"><span class="c3"></span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for(i=-s; i&lt;=s; i++)</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for(j=-s; j&lt;=s; j++)</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sum+= (IMG[x+i][y+j] * w[i+s][j+s])</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ksum += (w[i+s][j+s])</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; end for j</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; end for i</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; q += 1</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; end while</span></p><p class="c5"><span class="c3"></span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; RAD[x][y] = s</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; TOTAL[x][y] = sum</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; for(i = -s; i &lt;= s; i++)</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for(j = -s; j&lt;=s; j++)</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; NORM[x+m][y+n] += (w[i+s][j+s]/ksum)</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; end for j</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; end for i</span></p><p class="c5"><span class="c3"></span></p><p class="c10"><span class="c3">&nbsp; &nbsp; end for y</span></p><p class="c10"><span class="c3">end for x</span></p><p class="c10"><span class="c3">#--------------------</span></p><p class="c5"><span class="c3"></span></p><p class="c10"><span class="c3">for(x=0; x&lt;Lx; x++)</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; for(y=0; y&lt; Ly; y++)</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; IMG[x][y] /= NORM[x][y]</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; end for y</span></p><p class="c10"><span class="c3">end for x</span></p><p class="c5"><span class="c3"></span></p><p class="c10"><span class="c3">#--------------------</span></p><p class="c10"><span class="c3">for(x=0; x&lt;Lx; x++)</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; for(y=0; y&lt;Ly; y++)</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; s = RAD[x][y]</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; sum = 0</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; ksum = 0</span></p><p class="c5"><span class="c3"></span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; for(i=-s; i &lt;=s; i++)</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for(j=-s; j&lt;=s j++)</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sum += (IMG[x+i][y+i]*w[i+s][j+s])</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ksum += w[i+s][j+s]</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; end for j</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; end for i</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; </span></p><p class="c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; OUT[x][y] = sum / ksum</span></p><p class="c10"><span class="c3">&nbsp; &nbsp; end for y</span></p><p class="c10"><span class="c3">end for x</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">See More: </span><span class="c13 c6 c2"><a class="c4" href="https://github.com/chriskelvinlee/nasa_chandra/blob/master/AlgorithmNotes.txt">Github</a></span></p><p class="c5"><span class="c6 c2"></span></p><p class="c5"><span class="c6 c2"></span></p><h3 class="c10"><a name="h.gmmamn1zspfs"></a><span>Solution - MPI-CUDA</span></h3><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c8 c2">What is the proposed solution?</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">Parallelize the existing method to adaptively smooth an image. The current implementation is a serial algorithm called dmimgadapt(). Each pixel in the image is independently smoothed until a given threshold is met. This is used to smooth high contrast ratio images: the goal is to apply little smoothing to bright objects and large smoothing to the background; with a need to preserve the total energy (or flux). The goal of the project is to modify the dmimgadapt() routine from C-code to a more general python implementation. </span></p><p class="c5"><span class="c6 c2"></span></p><p class="c5"><span class="c15 c2"></span></p><p class="c5"><span class="c9 c2 c36"></span></p><hr><p class="c5"><span class="c15 c2"></span></p><p class="c5"><span class="c15 c2"></span></p><hr><p class="c5"><span class="c15 c2"></span></p><h2 class="c10"><a name="h.7y9adig5v2wz"></a><span>II. Data</span></h2><h3 class="c10"><a name="h.a38yx9ycgqmx"></a><span>Instrumentation</span></h3><p class="c5"><span class="c8 c9 c2"></span></p><p class="c10"><span class="c6 c2">The specific Chandra instrument we will be focusing on in this project is the Advanced CCD Imaging Spectrometer (ACIS). This instrument is an array of charged coupled devices more commonely known as CCDs. This instrument is especially useful because it measures the energy of each incoming X-ray. The ACIS instrument is the most commonly used of the Observatory. Find more information regarding the ACIS instrument </span><span class="c20 c6 c2"><a class="c4" href="http://chandra.si.edu/about/science_instruments.html">here</a></span><span class="c6 c2">. </span></p><p class="c5"><span class="c6 c2"></span></p><h3 class="c10"><a name="h.vhb0wb22jfqk"></a><span>Our Datasets</span></h3><p class="c5"><span class="c8 c9 c2"></span></p><p class="c10"><span class="c6 c8 c2">2). Describe your data in detail: where did it come from, how did you acquire it, what does it mean, etc.</span></p><p class="c10"><span class="c6 c2">The data we are using comes from the publicly available observations of the </span><span class="c20 c6 c2"><a class="c4" href="http://goo.gl/p6W6v">Chandra X-Ray Observatory</a></span><span class="c6 c2">. Data collected from the observatory is considered proprietary for up to one year, then the data becomes public. One of the main methods to access the data is through the </span><span class="c20 c6 c2"><a class="c4" href="http://goo.gl/BYn5i">Chandra Data Archive</a></span><span class="c6 c2">. In our project&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;we used the following Chandra Observations, denoted by Obs ID (Observation Identification Number): </span></p><p class="c10"><span class="c6 c2">a. OBSID 114 </span></p><p class="c10"><span class="c6 c2">Object: </span><span class="c20 c6 c2"><a class="c4" href="http://goo.gl/ZVBkC">CAS A</a></span><span class="c6 c2">&nbsp;</span></p><p class="c10"><span class="c6 c2">b. OBSID 7417 </span></p><p class="c10"><span class="c6 c2">Object: Cluster in NGC 2071 </span></p><p class="c10"><span class="c6 c2">c. OBSID 321 </span></p><p class="c10"><span class="c6 c2">Object: NGC 4472 = M49 (Galaxy in Virgo) </span></p><p class="c10"><span class="c6 c2">d. OBSID 11759 </span></p><p class="c10"><span class="c6 c2">Object: J0417.5-1154 (Strong lensing cluster)</span></p><p class="c5"><span class="c6 c2"></span></p><h4 class="c10"><a name="h.yabxqp60psjx"></a><span>Obtaining the Data</span></h4><p class="c5"><span class="c8 c9 c2"></span></p><p class="c10"><span class="c6 c2">On the </span><span class="c20 c6 c2"><a class="c4" href="http://goo.gl/ZVBkC">Chandra Data Archive</a></span><span class="c6 c2">&nbsp;website we used the OBSID of the above observations in the &quot;Observation ID&quot; field. Then hit search. This brings up the matching observations. We then clicked the &quot;Primary Products&quot; radio button and then &quot;Add to Retrieval List&quot;. We then selected &quot;Retrieve Products&quot;. We then entered my email address to be notified when the observation data was available. When the email arrived we followed the instructions on using anonymous ftp and the directory location of the data to retrieve the files. Once I had the files we uncompressed the file resulting in a directory with the Obs ID and a &quot;primary&quot; subdirectory. The data files we want are in the &quot;primary&quot; directory. For our project we used the files ending with evt2.fits. These are the primary event data that have gone through two levels of automated pipeline processing. This file contains all of the information regarding the data collected by ACIS that we used to create our input images.</span></p><p class="c5"><span class="c6 c2"></span></p><h4 class="c10"><a name="h.fbjskbfks9ov"></a><span>Preparing the Data</span></h4><p class="c5"><span class="c8 c9 c2"></span></p><p class="c10"><span class="c6 c2">In order to prepare the data, tools from the CIAO software suite were used. In order to install the software visit the </span><span class="c20 c6 c2"><a class="c4" href="http://goo.gl/dsE5M">CIAO </a></span><span class="c6 c2">website. The website gives detailed directions for downloading and installing the software. </span></p><p class="c10"><span class="c6 c2">Steps for Obs ID 321: </span></p><p class="c10"><span class="c6 c2">1. We wanted to convert the evt2.fits file into separate files for each ccd and bin the data by 8 using the</span><span class="c20 c6 c2"><a class="c4" href="http://goo.gl/V4POS">dmimg2jpg</a></span><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and </span><span class="c6 c2 c20"><a class="c4" href="http://goo.gl/x25ZH">dmkeypar</a></span><span class="c6 c2">&nbsp;tools from CIAO to determine which ccds were turned on during the observation. These are the command line calls we used: </span></p><p class="c10"><span class="c6 c2">%: dmkeypar &quot;acisf00321N003_evt2.fits.gz&quot; DETNAM echo+ </span></p><p class="c10"><span class="c6 c2">%: ACIS-235678 </span></p><p class="c10"><span class="c6 c2">%: dmimg2jpg &quot;acisf00321N003_evt2.fits.gz[ccd_id=2][bin sky=::8]&quot; out=321_ccd2.jpg scalefun=log </span></p><p class="c10"><span class="c6 c2">%: dmimg2jpg &quot;acisf00321N003_evt2.fits.gz[ccd_id=3][bin sky=::8]&quot; out=321_ccd3.jpg scalefun=log </span></p><p class="c10"><span class="c6 c2">%: dmimg2jpg &quot;acisf00321N003_evt2.fits.gz[ccd_id=5][bin sky=::8]&quot; out=321_ccd5.jpg scalefun=log </span></p><p class="c10"><span class="c6 c2">%: dmimg2jpg &quot;acisf00321N003_evt2.fits.gz[ccd_id=6][bin sky=::8]&quot; out=321_ccd6.jpg scalefun=log </span></p><p class="c10"><span class="c6 c2">%: dmimg2jpg &quot;acisf00321N003_evt2.fits.gz[ccd_id=7][bin sky=::8]&quot; out=321_ccd7.jpg scalefun=log </span></p><p class="c10"><span class="c6 c2">%: dmimg2jpg &quot;acisf00321N003_evt2.fits.gz[ccd_id=8][bin sky=::8]&quot; out=321_ccd8.jpg scalefun=log </span></p><p class="c10"><span class="c6 c2">2. We repeated the conversion for all four of our test Obs IDs. </span></p><p class="c10"><span class="c6 c2">3. Once we had a directory full of jpg images of all of the ccds for each of the observations I did a batch convert to turn them in png files. We did this using the MAC OS tool </span><span class="c20 c6 c2"><a class="c4" href="http://goo.gl/pGeFB">Automator</a></span><span class="c6 c2">&nbsp;to do the batch convert. These png images then became our input images for our python scripts.</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c5"><span class="c15 c2"></span></p><hr><p class="c5"><span class="c15 c2"></span></p><p class="c5"><span class="c15 c2"></span></p><hr><p class="c5"><span class="c15 c2"></span></p><p class="c5"><span class="c6 c2"></span></p><h2 class="c10"><a name="h.z8a6np292i4p"></a><span>III. Design</span></h2><p class="c5"><span class="c6 c2"></span></p><h3 class="c10"><a name="h.xrirg4e7pp4h"></a><span>Preprocessing</span></h3><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">In order to utilize the tools and methods we learned in this course, we wanted to use data images in the png file format. So we converted the Chandra data that original occurs in a fits file format to png files. For more about the data we used please visit the &quot;Data&quot; section of the website.</span></p><p class="c5"><span class="c6 c2"></span></p><h3 class="c10"><a name="h.573tyb65ad2a"></a><span>Serial Implementation</span></h3><p class="c10"><span class="c6 c2">Re-implemented serial version from original C source into Python</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c8 c2">3a). Describe your program design and why you chose the features you did.</span></p><p class="c5"><span class="c6 c2"></span></p><ol class="c22" start="1"><li class="c11 c10 c12"><span class="c6 c2">We wanted to use Python for seamless integration with the tools/ideas learned in the course. </span></li><li class="c11 c10 c12"><span class="c6 c2">The important features we chose to implement from the dmimgadapt() algorithm were the following: </span></li></ol><ol class="c31" start="1"><li class="c16 c10 c12"><span class="c6 c2">For each pixel in an input image, find the smallest box such that the sum of the pixels inside the box is at least some user defined threshold value. If the size of the box reaches a user defined maximum size, we move on to the next pixel. </span></li><li class="c16 c10 c12"><span class="c6 c2">Using the box size determined from (a), we create a properly normalized output image where the sum(input pixels) = sum(output pixels) </span></li></ol><ol class="c22" start="3"><li class="c11 c10 c12"><span class="c6 c2">Upon initially looking over the dmimgadapt() tool, we initially thought the serial algorithm would be implemented using MPI techniques similar to the Wave Equation Problem encountered in our HW3 problem set. But after further review of the algorithm, we noticed that the pixel values are static and therefore landed itself to GPU implementation. </span></li><li class="c11 c10 c12"><span class="c6 c2">We had to have 3 for-loop kernels to match dmimgadapt() source code. The Gaussian weights were relatively easy to implement in python in lines x and y of &#39;image_adapt_serial_gaussian.py.&#39;</span></li></ol><p class="c5"><span class="c15 c2"></span></p><h3 class="c10"><a name="h.lp9318555l"></a><span>GPU Implementation</span></h3><p class="c10"><span class="c6 c2">Implemented 3 CUDA kernels that accessed shared/global memory w/ MPI</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c8 c2">3b). Describe your program design and why you chose the features you did.</span></p><p class="c5"><span class="c6 c8 c2"></span></p><ol class="c22" start="1"><li class="c11 c10 c12"><span class="c6 c8 c2">pyCUDA</span><span class="c6 c2">: We decided to use pyCUDA to take advantage of the global memory and thread kernel computation</span></li></ol><ol class="c31" start="1"><li class="c16 c10 c12"><span class="c6 c2">3 for loops kernel made it ideal to implement into 3 CUDA kernels</span></li></ol><ol class="c22" start="2"><li class="c11 c10 c12"><span class="c6 c8 c2">CUDA</span><span class="c6 c2">: All IMG, BOX, NORM, OUT array were read from CPU to GPU once, all kernel&rsquo;s accessed the global memory, then only the OUT array was written from GPU to CPU to reduce expensive calls.</span></li></ol><ol class="c31" start="1"><li class="c16 c10 c12"><span class="c6 c2">Global memory: Each thread by block dimensions were 32x32 as allowable by machine setup, with nBx x nBy &nbsp;blocks, given the image dimension. Each thread read/wrote to IMG, BOX, NORM, OUT in global memory</span></li><li class="c16 c10 c12"><span class="c6 c2">Shared memory: Respective IMG, BOX, NORM, OUT data from global memory read into shared memory. Smoothing stencil sizes exceeding the bounds of shared memory were forced to use global memory.</span></li></ol><ol class="c22" start="3"><li class="c11 c10 c12"><span class="c6 c8 c2">Correctness</span><span class="c6 c2">: Pairs of serial and parallel output images were read and each pixel was cross-checked to ensure that the X-Ray source hit count was uncorrupted. Total relative error was computed over every single pixel.</span></li><li class="c11 c10 c12"><span class="c6 c8 c2">Speed-up</span><span class="c6 c2">: We measured speedup by measuring time in serial and parallel implementations. The time.time() cu.Event() libraries were used and summed independently for accurate measurement.</span></li><li class="c11 c10 c12"><span class="c6 c8 c2">MPI</span><span class="c6 c2">: To facilitate processing of a massively large dataset, we implemented the a MPI/CUDA solution to read in a maxiumum of 9 images at a time over the GPU. We did not try to process higher groups of images for fear of insufficient ranks.</span></li></ol><p class="c5"><span class="c2 c15"></span></p><hr><p class="c5"><span class="c15 c2"></span></p><p class="c5"><span class="c15 c2"></span></p><hr><p class="c5"><span class="c15 c2"></span></p><p class="c5"><span class="c6 c2"></span></p><h2 class="c10"><a name="h.vixq8tcovk5"></a><span>IV. Results</span></h2><h3 class="c10"><a name="h.abknyabiouc1"></a><span>Instructions</span></h3><h4 class="c10"><a name="h.nfmjhw9rc8kx"></a><span>Serial Instructions</span></h4><p class="c5"><span></span></p><p class="c10"><span class="c6 c8">4a). How do you use your application (mouse and keyboard functions, input/output, etc)?</span></p><p class="c10"><span class="c6 c2">In order to run the serial version we ended up having to hard code some of the user defined values as there seemed to be an issue reading in command line arguments on the Resonance Cluster. So near the top of the file is the &#39;file_name&#39; variable we use to to specify which image we wish to run through the algorithm. </span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">We then have our &#39;Threshold&#39; and &#39;MaxRad&#39; variables. These are the user defined variables that specify the value of the pixels in the created box, Threshold, and the maximum size box to attempt to reach the threshold value, MaxRad. </span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">The output image is created in the same directory as the input image. We remove the extension of the input file and then append &#39;_smoothed_serial.png&#39; to name. </span></p><p class="c10"><span class="c6 c2">While running our program on the Resonance cluster from the command line: </span></p><p class="c10"><span class="c6 c2">%: cd to base directory of our project </span></p><p class="c10"><span class="c6 c2">%: python image_adapt_serial_norm.py</span></p><p class="c5"><span class="c6 c2"></span></p><h4 class="c10"><a name="h.jjsrbbvdvg4b"></a><span>GPU</span><span>&nbsp;Instructions</span></h4><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c8">4b). How do you use your application (mouse and keyboard functions, input/output, etc)?</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">Aforementioned Threshold and MaxRad parameters apply in GPU version.</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">While running our program on the Resonance cluster from the command line: </span></p><p class="c10"><span class="c6 c2">%: gpu-login </span></p><p class="c10"><span class="c6 c2">%: module load packages/pycuda/2011.1.2</span></p><p class="c10"><span class="c6 c2">%: python </span><span class="c13 c6 c2"><a class="c4" href="http://image_adapt_gpu_global.py">image_adapt_gpu_global.py</a></span><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# to run global memory</span></p><p class="c10"><span class="c6 c2">%: python </span><span class="c13 c6 c2"><a class="c4" href="http://image_adapt_gpu.py">image_adapt_gpu.py</a></span><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c6 c2"># to run shared memory</span></p><p class="c10"><span class="c6 c2">%: mpiexec -n 5 python </span><span class="c13 c6 c2"><a class="c4" href="http://image_adapt_gpu.py">image_adapt_mpi.py</a></span><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c6 c2"># to run MPI/CUDA</span></p><p class="c10"><span class="c6 c2">%: python rel_error&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# to check for error</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">Change Debug flag to 0 or 1 to produce gpu_output.txt to compare with serial_output.txt. Relative error is outputted in rel_error.txt. Make sure all file locations are correct.</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c5"><span class="c6 c2"></span></p><h3 class="c10"><a name="h.3jl7us3v8pcw"></a><span>Image Results</span></h3><h4 class="c10"><a name="h.dhjvdw535yjt"></a><span>Sample Output</span></h4><p class="c5"><span class="c6 c8 c2"></span></p><p class="c10"><span class="c6 c8 c2">5a) </span><span class="c6 c8 c2">What is the performance of your code? </span></p><p class="c5"><span class="c2"></span></p><p class="c5"><span class="c2"></span></p><table cellpadding="0" cellspacing="0" class="c40"><tbody><tr><td class="c21"><p class="c29 c10"><img height="32" src="images/image00.png" width="32"><span class="c2"><br></span><span class="c1 c8">Fig 2a - 32x32 Original</span></p></td><td class="c21"><p class="c29 c10"><img height="32" src="images/image09.png" width="32"><span class="c2"><br></span><span class="c1 c8">Fig 2b - 32x32 GPU ASMOOTH</span></p></td></tr><tr><td class="c21"><p class="c29 c10"><img height="64" src="images/image12.png" width="64"><span class="c2"><br></span><span class="c1 c8">Fig 2c - 64x64 Original</span></p></td><td class="c21"><p class="c29 c10"><img height="64" src="images/image04.png" width="64"><span class="c2"><br></span><span class="c1 c8">Fig 2d - 64x64 GPU ASMOOTH</span></p></td></tr><tr><td class="c21"><p class="c29 c10"><img height="128" src="images/image13.png" width="128"><span class="c2"><br></span><span class="c1 c8">Fig 2e - 128x128 Original</span></p></td><td class="c21"><p class="c29 c10"><img height="128" src="images/image11.png" width="128"><span class="c2"><br></span><span class="c1 c8">Fig 2f - 128x128 GPU ASMOOTH</span></p></td></tr><tr><td class="c21"><p class="c29 c10"><img height="256" src="images/image07.png" width="256"><span class="c2"><br></span><span class="c1 c8">Fig 2h - 256x256 Original</span></p></td><td class="c21"><p class="c29 c10"><img height="256" src="images/image05.png" width="256"><span class="c2"><br></span><span class="c1 c8">Fig 2i - 256x256 GPU ASMOOTH</span></p></td></tr><tr><td class="c21"><p class="c29 c10"><img height="301" src="images/image01.png" width="301"><span class="c2"><br></span><span class="c1 c8">Fig 2j - 8192x8192 Original</span></p></td><td class="c21"><p class="c10 c29"><img height="303" src="images/image06.png" width="303"><span class="c1 c8">Fig 2k - 8192x8192 GPU ASMOOTH</span></p></td></tr></tbody></table><p class="c10"><span class="c1 c8">Figures 2a-k </span><span class="c1">Example output of CIAO 11759_ccd3 original images and adaptively smoothed images with CUDA with Threashold = 15 and RadMax = 4. Serial implementation in Fig 2j would have taken 4.5 days to compute. ASMOOTH in Fig 2k took 1.65 seconds. </span></p><p class="c5"><span class="c2"></span></p><h4 class="c10"><a name="h.blvldm2lahe0"></a><span>Speedup</span></h4><p class="c5"><span class="c2"></span></p><p class="c10"><span class="c6 c8 c2">5b) What speedup and efficiency did you achieve? </span></p><p class="c5"><span class="c2"></span></p><p class="c10"><span class="c6 c2">We measured a logarithmic speedup of 2.01-5.38x over the two datasets 11759_ccd3 and 11759. As expected, speedup grew increased linearly as dimensions grew from 32x32 to 512x512 pixels. However, the the upper pixel regime from 1024x1024 to 8192x8192 pixels, with 2^20 = 1,048,576 pixels and 2^26 = 67,108,864 pixels, we quickly observe Ahmdal&rsquo;s Law quickly take effect. Further measurements could have been made; however, the serial version was the bottleneck and took too much time to compute. Accuracy was verified with a </span><span class="c32 c6 c2"><a class="c4" href="http://rel_error.py">rel_error.py</a></span><span class="c6 c2">&nbsp;script to ensure pixel representing X-Ray source hit count was accurate.</span></p><p class="c5"><span class="c2"></span></p><table cellpadding="0" cellspacing="0" class="c40"><tbody><tr><td class="c7"><p class="c17 c29 c10"><img height="395" src="images/image03.png" width="526"><span class="c2"><br></span><span class="c1 c8">Fig 3 - </span><span class="c1">Speedup plot (log) over pixel dimensions. Note that x-axis and y-axis labels plotted<br>on normalized intervals for readability and not adjusted for true scale.</span></p></td></tr></tbody></table><p class="c5"><span class="c2"></span></p><p class="c5"><span class="c2"></span></p><p class="c10"><span class="c8 c9">Error Checking</span></p><p class="c5"><span class="c2"></span></p><table cellpadding="0" cellspacing="0" class="c40"><tbody><tr><td class="c28"><p class="c14 c10"><span class="c6 c2">Image Dimensions</span></p></td><td class="c27"><p class="c14 c10"><span class="c6 c2">Relative Pixel Error</span></p></td></tr><tr><td class="c28"><p class="c14 c10"><span class="c6 c2">32 x 32</span></p></td><td class="c27"><p class="c14 c10"><span class="c6 c2">8.730360e-05</span></p></td></tr><tr><td class="c28"><p class="c14 c10"><span class="c6 c2">64 x 64</span></p></td><td class="c27"><p class="c14 c10"><span class="c6 c2">7.014218e-05</span></p></td></tr><tr><td class="c28"><p class="c10 c14"><span class="c6 c2">128 x 128</span></p></td><td class="c27"><p class="c14 c10"><span class="c6 c2">1.299115e-05</span></p></td></tr><tr><td class="c28"><p class="c14 c10"><span class="c6 c2">256 x 256</span></p></td><td class="c38"><p class="c14 c10"><span class="c6 c2">8.053484e-06</span></p></td></tr><tr><td class="c28"><p class="c14 c10"><span class="c6 c2">512 x 512</span></p></td><td class="c27"><p class="c14 c10"><span class="c6 c2">1.411130e-05</span></p></td></tr><tr><td class="c28"><p class="c14 c10"><span class="c6 c2">&hellip; </span></p></td><td class="c27"><p class="c14 c10"><span class="c6 c2">Serial version takes 2.5+ days</span></p></td></tr></tbody></table><p class="c5"><span class="c2"></span></p><p class="c10"><span class="c6 c2">Apart from marginal rounding error in float32, serial and parallel adaptively smoothed mages were verified to be identical and have relative pixel errors less than 1e-05.</span></p><p class="c5"><span class="c2"></span></p><p class="c10"><span class="c8 c9">Optimizations</span></p><p class="c5"><span class="c2"></span></p><p class="c10"><span class="c6 c8 c2">5c.) What optimizations did you implement to achieve this speedup?</span></p><p class="c5"><span class="c6 c8 c2"></span></p><ol class="c30" start="1"><li class="c11 c10 c12"><span class="c6 c2">Minimize number of read/writes from host to device</span></li></ol><p class="c5"><span class="c6 c8 c2"></span></p><p class="c11 c10"><span class="c1"># Copy arrays from host to device once</span></p><p class="c11 c10"><span class="c1">IMG_device &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= gpuarray.to_gpu(IMG)</span></p><p class="c11 c10"><span class="c1">BOX_device &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= gpuarray.to_gpu(BOX)</span></p><p class="c11 c10"><span class="c1">NORM_device &nbsp; &nbsp; &nbsp; &nbsp; = gpuarray.to_gpu(NORM)</span></p><p class="c11 c10"><span class="c1">OUT_device &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= gpuarray.to_gpu(OUT)</span></p><p class="c5 c11"><span class="c1"></span></p><p class="c11 c10"><span class="c1"># Copy image once from device to host</span></p><p class="c11 c10"><span class="c1">IMG_out = OUT_device.get()</span></p><p class="c5"><span class="c6 c8 c2"></span></p><ol class="c30" start="1"><li class="c11 c10 c12"><span class="c6 c2">Favor global memory over shared memory [see image_adapt_gpu_global.py]</span></li></ol><p class="c5"><span class="c2"></span></p><p class="c16 c10"><span class="c1">sum += IMG[gtid + ii*Ly + jj];&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[line 102, r]</span></p><p class="c16 c10"><span class="c1">NORM[gtid + ii*Ly + jj] += &nbsp;1.0 / ksum;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[line 121, w]</span></p><p class="c16 c10"><span class="c1">IMG[gtid] /= NORM[gtid];&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[line 161, w]</span></p><p class="c16 c10"><span class="c1">OUT[gtid] = sum / ksum;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[line 211, w]</span></p><p class="c5"><span class="c2"></span></p><ol class="c30" start="2"><li class="c11 c10 c12"><span class="c6 c2">Use MPI to run multiple CUDA jobs (not a speedup improvement, but a workflow improvement)</span></li></ol><p class="c5"><span class="c8 c24"></span></p><p class="c16 c10"><span class="c3">comm = MPI.COMM_WORLD</span></p><p class="c10 c16"><span class="c3">size = comm.Get_size()</span></p><p class="c16 c10"><span class="c3">rank = comm.Get_rank()</span></p><p class="c5 c16"><span class="c3"></span></p><p class="c5 c16"><span class="c3"></span></p><p class="c16 c10"><span class="c3">file_set0 = [&#39;extrap_data/11759_ccd3/11759_32x32.png&#39;,</span></p><p class="c16 c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &#39;extrap_data/11759_ccd3/11759_64x64.png&#39;,</span></p><p class="c16 c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &#39;extrap_data/11759_ccd3/11759_128x128.png&#39;,</span></p><p class="c16 c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &#39;extrap_data/11759_ccd3/11759_256x256.png&#39;,</span></p><p class="c16 c10"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &#39;extrap_data/11759_ccd3/11759_512x512.png&#39;]</span></p><p class="c5 c16"><span class="c3"></span></p><p class="c16 c10"><span class="c3">parallel_smooth(file_set0[rank], rank, size, comm)</span></p><p class="c5 c16"><span class="c3"></span></p><p class="c5"><span class="c2"></span></p><h3 class="c10"><a name="h.c65oqio4bjn"></a><span>Discussion</span></h3><h4 class="c10"><a name="h.be8ny55ll768"></a><span>Insights</span></h4><p class="c5"><span class="c2"></span></p><p class="c10"><span class="c6 c8 c2">6.) What interesting insights did you gain from this project?</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c8 c2">Danny</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">I know have a deeper understanding of the dmimgadapt() CIAO tool. Before this project I had never used it before at work. I now see the great benefits of using GPUs to analyze real life data. When I enrolled in this course I wasn&#39;t exactly sure how it would translate to the types of data I deal with and algorithms we implement at work, now I see how much more quickly and efficiently things can be handled with parallel processing. Whether it comes in the form of GPU or MPI, there is great benefit from utilizing these approaches in the astronomy field.</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c8 c2">Christopher</span></p><p class="c5"><span class="c6 c2"></span></p><ol class="c22" start="1"><li class="c11 c10 c12"><span class="c6 c2">Shared memory is both more difficult to implementation and prone to memory access error. In this case, the size of the adaptive smoothing kernel is dynamic and may change from n=9, thus all pixels 9 from the border of a 32x32 block will need to be computed on global memory, or shared memory will need to get have 9-pixel padding effectively reducing the size of the block to 14x14 (32 - 18 = 14). At this point, it&rsquo;s not worthwhile to pursue a shared memory implementation due to the added complexity and added cost of initializing the read/write of shared memory from and to global memory.</span></li></ol><p class="c5"><span class="c6 c2"></span></p><ol class="c22" start="2"><li class="c11 c10 c12"><span class="c6 c2">Global memory seems to be actually faster for the reasons explained in 1), but also because global memory bandwidth seems to be sufficient for our purposes here. Our 3 CUDA kernels are not computationally expensive and will run relatively fast, even if memory speed could be improved with shared memory. With speeds &gt;1.65 seconds for 8192x8192 pixels, global memory is sufficient for the astronomy use</span></li></ol><p class="c5"><span class="c6 c2"></span></p><ol class="c22" start="3"><li class="c11 c10 c12"><span class="c6 c2">There seems to be some background noise/unknown anomaly that creates very, very faint white ghost streaks in the gpu versions. It is most likely a misunderstanding of the dmimgadapt() CIAO tool rather than an indexing error. The so-called rounding error is minimal enough that it can be ignored.</span></li></ol><p class="c5"><span class="c6 c2"></span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c8">Extensions</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c8 c2">7.) Extensions and improvements can you suggest?</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">While implementing the project we ran into difficulty implementing a weighted sum approach. We had a serial version coded up utilizing a gaussian weighted sum, then when we were coding the parallel version we ran into difficulties. What took two lines to achieve in Python, was seemingly going to take 15-20 lines. So we deferred that as an item we would like to implement given more time. In addition to the gaussian weighted sum, there are other methods we could implement, like cone, boxcar, tophat, or mexican hat weighted sum approaches. In order to make the input files easier for us to deal with in python and with the installed packages on Resonance, we decided to convert the astronomical data from fits files ultimately to png files. An improvement we could deliver would be to handle the fits file format directly, read in the data, perform our adaptive smoothing algorithm, then write out a new fits file. This would also allow for a more one to one comparison between the C code implementation from CIAO tools to the Python implementation we used. The original C implementation has the option to output some of the intermediate data files. The user can specify to output a file of all of the sum values, a file of all the normalization values and also all the radius/box size values. The ultimate goal of the project would be to incorporate it into the CIAO data model. This would allow for seamless integration into data analysis for x-ray astronomers with access to a GPU. The CIAO Data Model (</span><span class="c20 c6 c2"><a class="c4" href="http://goo.gl/QMXx0">DM</a></span><span class="c6 c2">) is a versatile interface used by CIAO to examine and manipulate standard format data files (e.g Fits, ascii). This would greatly reduce the time for data analysis.</span></p><p class="c5"><span class="c6 c2"></span></p><hr><p class="c5"><span class="c15 c2"></span></p><p class="c5"><span class="c15 c2"></span></p><hr><p class="c5"><span class="c15 c2"></span></p><p class="c5"><span class="c6 c2"></span></p><h2 class="c10"><a name="h.59v0s8xxwkqs"></a><span>V. Conclusion</span></h2><p class="c5"><span class="c6 c2"></span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c8 c2">8.) What did you most enjoy about working on this project? What was the most challenging aspect? What was the most frustrating? What would you do differently next time?</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c8 c2">Danny</span></p><p class="c5"><span class="c2 c6"></span></p><p class="c10"><span class="c6 c2">I enjoyed being able to apply the concepts learned in the course to things I see at work. Many times in past courses I have taken, the projects end up being more of an academic exercise than real life applications. I see now why the instructors requesting people pick something they found interesting to work on because I did spent a lot of time on it and it was nice to know I can apply this knowledge to future projects outside of the course environment.</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">The most challenging aspect for me was to translate the existing C code with all of the specific &quot;CIAOisms&quot; and create a Python version of the code. Luckily, my boss, Kenny Glotfelty, provided a basic pseudo code representation of the algorithm. However, there were still challenges creating a pythonic version of a c algorithm. I am sure there are more pythonic ways to do some of the computations we ended up doing. So part of the frustration was learning the ins and outs of Python and realizing that somethings are not meant to have a direct c to python conversion. There are times when Python does things differently and it took time to learn and figure out what those things are.</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">One of the most frustrating things that I came across ended up being a simple issue of the type of input file we were using. CIAO provided us a tool called dmimg2jpg, so I used that to create what I thought was a useful input jpg image. I thought I had the serial version of the code working wonderfully but ended up with a terrible output image. So I went through the algorithm line by line. When I got to the top I thought that surely the input file must be ok, but they way I was reading the file, the type I needed was a png file, the jpg version was not giving the same values as the png version. So many hours were spent looking into the problem and many tears of joy occurred when the solution was found. It always seems that the hardest bugs are the smallest ones at the end to diagnose and deal with.</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">I think the thing that I would do differently next time would be to have a better overall understanding of the project I was dealing with going into it. I didn&#39;t realize the complexity of the original algorithm and assumed the time window we had for the final project was ample time to work through it all. Had I known the specific complexities of dealing with C and Python in the same time, I think I might not have bitten of more than I could chew. I think I also would have spent more time at the beginning getting things set up to use the fits file format so we would have had the exact same data to work with as the C program. Then we could have had a direct comparison between the C algorithm and the GPU algorithm, instead of a more generic Python version of the algorithm.</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c8 c2">Christopher</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">This was a very enjoyable final project with the right amount of difficulty. I am surprised by my own ability to implement serial algorithms into parallel versions with fair proficiency. I especially enjoyed learning more about astronomy and pushing what I had learned in class to complete tasks that probably haven&rsquo;t been done before. Thinking through how to implement 3 CUDA kernels, then optimizing work-flow with MPI+CUDA, really forced me to learn and think about the course material very well. </span></p><p class="c5"><span class="c6 c2"></span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">Scientific computing is a challenging area of study that has wide-implications. My only cautionary advice that I&rsquo;d give myself: shared memory can sometimes cause more headaches than solutions, especially in cases where memory r/w are sparse to begin with.</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c5"><span class="c6 c2"></span></p><hr><p class="c5"><span class="c15 c2"></span></p><p class="c5"><span class="c15 c2"></span></p><hr><p class="c5"><span class="c6 c2"></span></p><p class="c5"><span class="c6 c2"></span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c8">Contact</span></p><p class="c5"><span class="c6"></span></p><p class="c10"><span class="c6 c2">Danny Gibbs</span></p><p class="c10"><span class="c6 c2">Harvard-Smithsonian Center for Astrophysics</span></p><p class="c10"><span class="c6 c2">dgibbs [at] head.cfa.harvard.edu </span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">Christopher Lee</span></p><p class="c10"><span class="c6 c2">Harvard University</span></p><p class="c10"><span class="c6 c2">cklee [at] college.harvard.edu </span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c8 c2">About</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">Danny Gibbs works for the Harvard-Smithsonian Center for Astrophysics in Cambridge, MA. He has been focusing on application software for data analysis/reduction for the Chandra X-Ray Observatory for almost 5 years. The large data sets that astronomical data provides are very applicable to this course. He is taking this class to advance his understanding of parallel computing to bring to the job site.</span></p><p class="c5"><span class="c6 c2"></span></p><p class="c10"><span class="c6 c2">Christopher K. Lee is a student at the College studying Applied Mathematics. He is interested in applying to CS graduate school, and would like to get more exposure to the sciences. Christopher&#39;s father is a head engineer for Northrup Grumman and introduced him to space technologies while growing up.</span></p><p class="c5"><span></span></p><p class="c10"><span>View the speedup calculations </span><span class="c32"><a class="c4" href="https://docs.google.com/spreadsheet/ccc?key=0Age4mtYREqYgdFVNZnpIaEVGenE0dWM3YnZobGtvdkE&amp;hl=en_US#gid=0">here</a></span><span>.</span></p><p class="c5"><span></span></p><div><p class="c5 c34"><span></span></p></div></body></html>